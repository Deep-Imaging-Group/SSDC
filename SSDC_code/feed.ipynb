{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from six.moves import xrange\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import cnn as model\n",
    "import config\n",
    "import input_data\n",
    "import data_producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_float('learning_rate', 0.0003, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('num_epochs', 400, 'Number of epochs to run trainer.')\n",
    "flags.DEFINE_integer('batch_size', 32, 'Batch size.  '\n",
    "                     'Must divide evenly into the dataset sizes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = config.patch_size\n",
    "CHANNELS = config.channels\n",
    "NUM_CLASSES = config.num_classes\n",
    "\n",
    "reduce_lr_epoch_1 = 200\n",
    "reduce_lr_epoch_2 = 300\n",
    "\n",
    "\n",
    "TRAIN_FILES = 8\n",
    "TEST_FILES = 8\n",
    "DATA_PATH = os.path.join(os.getcwd(),\"Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def placeholder_inputs(batch_size):\n",
    "    \"\"\"Generate placeholder variables to represent the input tensors.\n",
    "    These placeholders are used as inputs by the rest of the model building\n",
    "    code and will be fed from the downloaded data in the .run() loop, below.\n",
    "    Args:\n",
    "    batch_size: The batch size will be baked into both placeholders.\n",
    "    Returns:\n",
    "    images_placeholder: Images placeholder.\n",
    "    labels_placeholder: Labels placeholder.\n",
    "    is_training: Is_training placeholder.\n",
    "    learning_rate: Learning_rate placeholder.\n",
    "    \"\"\"\n",
    "    # Note that the shapes of the placeholders match the shapes of the full\n",
    "    # image and label tensors, except the first dimension is now batch_size\n",
    "    # rather than the full size of the train or test data sets.\n",
    "    images_placeholder = tf.placeholder(tf.float32, shape=(None, model.IMAGE_PIXELS))\n",
    "    labels_placeholder = tf.placeholder(tf.int32, shape=(None))\n",
    "    is_training = tf.placeholder(dtype=tf.bool)\n",
    "    learning_rate = tf.placeholder(tf.float32)\n",
    "    return images_placeholder, labels_placeholder, is_training, learning_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_feed_dict(data_set, batch_size, images_pl, labels_pl, is_training, TF, learning_rate):\n",
    "    \"\"\"Fills the feed_dict for training the given step.\n",
    "    A feed_dict takes the form of:\n",
    "    feed_dict = {\n",
    "      <placeholder>: <tensor of values to be passed for placeholder>,\n",
    "      ....\n",
    "    }\n",
    "    Args:\n",
    "    data_set: The set of images and labels, from input_data.read_data_sets()\n",
    "    batch_size: The batch size will be baked into both placeholders.\n",
    "    images_pl: The images placeholder, from placeholder_inputs().\n",
    "    labels_pl: The labels placeholder, from placeholder_inputs().\n",
    "    is_training: The is_training placeholder, from placeholder_inputs().\n",
    "    TF: Training or not.\n",
    "    learning_rate: The learning_rate placeholder, from placeholder_inputs().\n",
    "    Returns:\n",
    "    feed_dict: The feed dictionary mapping from placeholders to values.\n",
    "    labels_feed: The next batch label examples.\n",
    "    \"\"\"\n",
    "    # Create the feed_dict for the placeholders filled with the next\n",
    "    # `batch size ` examples.\n",
    "    images_feed, labels_feed = data_set.next_batch(batch_size)\n",
    "    feed_dict = {\n",
    "      images_pl: images_feed,\n",
    "      labels_pl: labels_feed,\n",
    "      is_training: TF,\n",
    "      learning_rate: FLAGS.learning_rate,\n",
    "    }\n",
    "    return feed_dict, labels_feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_eval(sess,\n",
    "            predict,\n",
    "            images_placeholder,\n",
    "            labels_placeholder,\n",
    "            is_training,\n",
    "            learning_rate,\n",
    "            data_set):\n",
    "    \"\"\"Runs one evaluation against the full epoch of data.\n",
    "    Args:\n",
    "    sess: The session in which the model has been trained.\n",
    "    predict: The Tensor that returns the prediction result of logits.\n",
    "    images_placeholder: The images placeholder.\n",
    "    labels_placeholder: The labels placeholder.\n",
    "    is_training: The is_training placeholder.\n",
    "    learning_rate: The learning_rate placeholder.\n",
    "    data_set: The set of images and labels to evaluate, from\n",
    "      input_data.read_data_sets().\n",
    "    \"\"\"\n",
    "    matrix = np.zeros((NUM_CLASSES,NUM_CLASSES))\n",
    "    # And run one epoch of eval.\n",
    "    steps_per_epoch = data_set.num_examples // FLAGS.batch_size\n",
    "    num_rest_examples = data_set.num_examples - steps_per_epoch * FLAGS.batch_size\n",
    "    for step in xrange(steps_per_epoch):\n",
    "        feed_dict, labels_feed = fill_feed_dict(data_set,\n",
    "                                   FLAGS.batch_size,\n",
    "                                   images_placeholder,\n",
    "                                   labels_placeholder,\n",
    "                                   is_training,\n",
    "                                   False,\n",
    "                                   learning_rate,)\n",
    "        prediction = sess.run(predict, feed_dict=feed_dict)\n",
    "\n",
    "        for i in range(FLAGS.batch_size):\n",
    "            matrix[int(labels_feed[i])][prediction[i]] += 1\n",
    "    \n",
    "    if ( num_rest_examples != 0 ):\n",
    "        feed_dict, labels_feed = fill_feed_dict(data_set,\n",
    "                                   num_rest_examples,\n",
    "                                   images_placeholder,\n",
    "                                   labels_placeholder,\n",
    "                                   is_training,\n",
    "                                   False,\n",
    "                                   learning_rate,)\n",
    "        prediction = sess.run(predict, feed_dict=feed_dict)\n",
    "        for i in range(num_rest_examples):\n",
    "            matrix[int(labels_feed[i])][prediction[i]] += 1\n",
    "            \n",
    "    oa,aa,kappa,prec = calculate_precision(matrix)\n",
    "    return oa,aa,kappa,prec\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision(matrix):\n",
    "    matrix_sum_col = np.zeros(NUM_CLASSES)\n",
    "    matrix_sum_row = np.zeros(NUM_CLASSES)\n",
    "    diagonal_sum = 0\n",
    "    total_num = 0\n",
    "    for i in range(NUM_CLASSES):\n",
    "        diagonal_sum += matrix[i][i]\n",
    "        for j in range(NUM_CLASSES):\n",
    "            matrix_sum_col[j] += matrix[i][j]\n",
    "            matrix_sum_row[i] += matrix[i][j]\n",
    "            total_num += matrix[i][j]\n",
    "\n",
    "    oa = diagonal_sum / total_num\n",
    "    aa = 0\n",
    "    prec = 'class#\\tright\\ttest\\trate\\n'\n",
    "    for i in range(NUM_CLASSES):\n",
    "        prec_per_class = matrix[i][i] / matrix_sum_row[i]\n",
    "        aa += prec_per_class\n",
    "        prec = prec + '%d\\t%d\\t%d\\t%.5f\\n' % (i+1, int(matrix[i][i]), int(matrix_sum_row[i]),prec_per_class)\n",
    "    aa = aa / NUM_CLASSES\n",
    "\n",
    "    kappa_temp = 0\n",
    "    for i in range(NUM_CLASSES):\n",
    "        kappa_temp += matrix_sum_col[i] * matrix_sum_row[i]\n",
    "    kappa = (total_num*diagonal_sum-kappa_temp) / (total_num*total_num-kappa_temp)\n",
    "   \n",
    "    print('  OA: %.5f' % oa)\n",
    "    print('  AA: %.5f' % aa)\n",
    "    print('  kappa: %.5f' % kappa)\n",
    "\n",
    "    return oa,aa,kappa, prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(sess,\n",
    "                    train_op,\n",
    "                    loss,\n",
    "                    images_placeholder,\n",
    "                    labels_placeholder,\n",
    "                    is_training,\n",
    "                    learning_rate,\n",
    "                    data_set):\n",
    "\n",
    "    steps_per_epoch = data_set.num_examples // FLAGS.batch_size\n",
    "    total_loss = []\n",
    "    for step in xrange(steps_per_epoch):\n",
    "        feed_dict,_ = fill_feed_dict(data_set,\n",
    "                                   FLAGS.batch_size,\n",
    "                                   images_placeholder,\n",
    "                                   labels_placeholder,\n",
    "                                   is_training,\n",
    "                                   True,\n",
    "                                   learning_rate)\n",
    "        _, loss_value = sess.run([train_op, loss],\n",
    "                                   feed_dict=feed_dict)\n",
    "        total_loss.append(loss_value)\n",
    "    mean_loss = np.mean(total_loss)\n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_DataSet(first,second):\n",
    "    temp_image = np.concatenate((first.images,second.images),axis=0)\n",
    "    temp_labels = np.concatenate((first.labels,second.labels),axis=0)\n",
    "    temp_image = temp_image.reshape(temp_image.shape[0],IMAGE_SIZE,IMAGE_SIZE,CHANNELS)\n",
    "    temp_image = np.transpose(temp_image,(0,3,1,2))\n",
    "    temp_labels = np.transpose(temp_labels)\n",
    "    return input_data.DataSet(temp_image,temp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trainable_params():\n",
    "    total_parameters = 0\n",
    "    for variable in tf.trainable_variables():\n",
    "        shape = variable.get_shape()\n",
    "        variable_parametes = 1\n",
    "        for dim in shape:\n",
    "            variable_parametes *= dim.value\n",
    "        total_parameters += variable_parametes\n",
    "    print(\"  Total training params: %.1fK\" % (total_parameters / 1e3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_training_and_testing_dataset():\n",
    "    \"\"\"Concatenating all the training and test mat files\"\"\"\n",
    "    for i in range(TRAIN_FILES):\n",
    "        data_sets = input_data.read_data_sets(os.path.join(DATA_PATH, config.dataset+'_Train_'+str(IMAGE_SIZE)+'_'+str(i+1)+'.mat'), 'train')\n",
    "        if(i==0):\n",
    "            Training_data = data_sets\n",
    "            continue\n",
    "        else:\n",
    "            Training_data = add_DataSet(Training_data,data_sets)\n",
    "            \n",
    "    for i in range(TEST_FILES):\n",
    "        data_sets = input_data.read_data_sets(os.path.join(DATA_PATH, config.dataset+'_Test_'+str(IMAGE_SIZE)+'_'+str(i+1)+'.mat'),'test')\n",
    "        if(i==0):\n",
    "            Test_data = data_sets\n",
    "            continue\n",
    "        else:\n",
    "            Test_data = add_DataSet(Test_data,data_sets)\n",
    "    return Training_data, Test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(_):\n",
    "    \"\"\"Train and evaluate model for a number of times.\"\"\"\n",
    "        \n",
    "    # Tell TensorFlow that the model will be built into the default Graph.\n",
    "    with tf.Graph().as_default():\n",
    "        # Generate placeholders for the images, labels, is_training and learning_rate.\n",
    "        images_placeholder, labels_placeholder, is_training, learning_rate = placeholder_inputs(FLAGS.batch_size)\n",
    "\n",
    "        # Build a Graph that computes predictions from the inference model.\n",
    "        logits = model.inference(images_placeholder, is_training)\n",
    "\n",
    "        # Add to the Graph the Ops for loss calculation.\n",
    "        loss = model.loss(logits, labels_placeholder)\n",
    "\n",
    "        # Add to the Graph the Ops that calculate and apply gradients.\n",
    "        train_op = model.training(loss, learning_rate)\n",
    "\n",
    "        # Add the Op to calculate prediction result of logits..\n",
    "        predict = model.predicting(logits)\n",
    "\n",
    "        sess = tf.Session()\n",
    "        \n",
    "        saver = tf.train.Saver(max_to_keep=1000)\n",
    "\n",
    "        training_times = 10\n",
    "        for times_counter in xrange(training_times):\n",
    "            data_producer.create_dataset()\n",
    "            Training_data, Test_data = concat_training_and_testing_dataset()\n",
    "            \n",
    "\n",
    "            init = tf.global_variables_initializer()\n",
    "            sess.run(init)\n",
    "            count_trainable_params()\n",
    "            FLAGS.learning_rate = 0.0003\n",
    "            duration1 = 0\n",
    "            train_duration = 0\n",
    "            test_duration = 0\n",
    "            train_oa = 0\n",
    "            train_aa = 0\n",
    "            train_kappa = 0\n",
    "            train_loss = 0\n",
    "            train_epoch = 0\n",
    "            \n",
    "            # Start the training loop.\n",
    "            for epoch in xrange(FLAGS.num_epochs):\n",
    "                train_start_time = time.time()\n",
    "\n",
    "                if epoch == reduce_lr_epoch_1 or epoch == reduce_lr_epoch_2:\n",
    "                    FLAGS.learning_rate = FLAGS.learning_rate / 10\n",
    "                    #print(\"Decrease learning rate, new lr = %f\" % FLAGS.learning_rate)\n",
    "                \n",
    "                one_epoch_loss = train_one_epoch(sess,\n",
    "                                                 train_op,\n",
    "                                                 loss,\n",
    "                                                 images_placeholder,\n",
    "                                                 labels_placeholder,\n",
    "                                                 is_training,\n",
    "                                                 learning_rate,\n",
    "                                                 Training_data)\n",
    "                duration1 += (time.time() - train_start_time)\n",
    "                \n",
    "\n",
    "                if (epoch + 1) % 20 == 0 or (epoch + 1) == FLAGS.num_epochs:\n",
    "                    test_start_time = time.time()\n",
    "                    oa,aa,kappa,prec = do_eval(sess,\n",
    "                                        predict,\n",
    "                                        images_placeholder,\n",
    "                                        labels_placeholder,\n",
    "                                        is_training,\n",
    "                                        learning_rate,\n",
    "                                        Test_data)\n",
    "                    duration2 = (time.time() - test_start_time)\n",
    "\n",
    "                    if oa > train_oa:\n",
    "                        train_oa = oa\n",
    "                        train_aa = aa\n",
    "                        train_kappa = kappa\n",
    "                        train_duration = duration1\n",
    "                        test_duration = duration2\n",
    "                        train_loss = one_epoch_loss\n",
    "                        train_epoch = epoch\n",
    "                        train_prec = prec\n",
    "                        saver.save(sess, DATA_PATH+'/record/'+config.dataset+'/cnn_t' + str(times_counter) + '-' + str(IMAGE_SIZE)+'X'+str(IMAGE_SIZE)+'.ckpt', global_step=epoch)\n",
    "\n",
    "            print('-------------------Training %d-----------------' % times_counter)\n",
    "            print('  OA: %.5f' % train_oa)\n",
    "            print('  AA: %.5f' % train_aa)\n",
    "            print('  kappa: %.5f' % train_kappa)\n",
    "            print('  Epoch: %d' % train_epoch)\n",
    "            print('  Training time spent: %.4f s' % train_duration)\n",
    "            print('  Testing time spent: %.4f s' % test_duration)\n",
    "            print('  Loss: %.4f' % train_loss)\n",
    "            #print(prec)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
